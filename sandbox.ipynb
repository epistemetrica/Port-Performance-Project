{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prelims\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import time\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "#enable string cache for polars categoricals\n",
    "pl.enable_string_cache()\n",
    "#display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pl.Config(tbl_rows=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init list of lazyframes\n",
    "lfs = []\n",
    "#process each parquet file individually into lazyframes\n",
    "for file in glob.glob('ais data/data/ais_clean/*.parquet'):\n",
    "    try:\n",
    "        #check file integrity \n",
    "        pl.scan_parquet(file).collect_schema()\n",
    "        #read file\n",
    "        lf = (\n",
    "            pl.scan_parquet(file)\n",
    "            #drop smaller vessels\n",
    "            .filter(pl.col('length')>100)\n",
    "            #sort by vessel and time\n",
    "            .sort(['mmsi', 'time'])\n",
    "            #indicate whether status is the same as previous row (Fill value needed to avoid status 0 evaluating as equal to false)\n",
    "            #.with_columns(\n",
    "            #    status_change = (\n",
    "            #        pl.col('status').ne(pl.col('status').shift(fill_value=20))\n",
    "            #        .over('mmsi')\n",
    "            #    ),\n",
    "            #    status_previous = pl.col('status').shift().over('mmsi')\n",
    "            #)\n",
    "            #keep only new status pings\n",
    "            #.filter(pl.col('status_change')==True)\n",
    "            #drop change col\n",
    "            #.drop('status_change')\n",
    "        )\n",
    "        #append to list of lazyframes\n",
    "        lfs.append(lf)\n",
    "    except:\n",
    "        print(f'{file} failed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1769422121"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf = pl.concat(lfs, how='diagonal_relaxed')\n",
    "lf.select(pl.len()).collect().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.select(pl.len()).collect().item() - lf.unique(subset=['mmsi', 'time']).select(pl.len()).collect().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/adamwilson/Downloads/')\n",
    "\n",
    "def bad_lines(line):\n",
    "    line = line[1:]\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('AIS_2023_05_09.csv', engine='python', on_bad_lines=bad_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's happening in April 2023??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(\n",
    "    monthly_df\n",
    "    .group_by('month').agg(pl.col('vessels_avg').sum())\n",
    "    .sort('month'),\n",
    "    y='vessels_avg', x='month',\n",
    "    title='Vessels per month',\n",
    "    labels={'vessels_avg':'Vessel Count'}\n",
    ")\n",
    "\n",
    "daily_df = (\n",
    "    #convert to polars\n",
    "    pl.DataFrame(main_gdf.drop(['geometry', 'geometry_port'], axis=1))\n",
    "    #create day column\n",
    "    .with_columns(day = pl.col('time').dt.strftime('%Y%m%d'))\n",
    "    #agg over ports and days\n",
    "    .group_by('port_name', 'day')\n",
    "    .agg(\n",
    "        #keep lat and long\n",
    "        lat = pl.col('lat').first(),\n",
    "        lon = pl.col('lon').first(),\n",
    "        #get monthly avg vessels\n",
    "        vessels_avg = pl.col('mmsi').n_unique(),\n",
    "        #get average time at berth\n",
    "        time_at_berth_avg = (\n",
    "            pl.when(pl.col('status')==5)\n",
    "            .then(pl.col('status_duration'))\n",
    "            .otherwise(pl.lit(None))\n",
    "        ).median()/60,\n",
    "        #get average time at anchor\n",
    "        time_at_anchor_avg = (\n",
    "            pl.when(pl.col('status')==1)\n",
    "            .then(pl.col('status_duration'))\n",
    "            .otherwise(pl.lit(None))\n",
    "        ).median()/60\n",
    "    )\n",
    ")\n",
    "\n",
    "px.bar(\n",
    "    daily_df\n",
    "    .filter(\n",
    "        pl.col('day').str.starts_with('202302') | \n",
    "        pl.col('day').str.starts_with('202303') | \n",
    "        pl.col('day').str.starts_with('202304') |\n",
    "        pl.col('day').str.starts_with('202305') |\n",
    "        pl.col('day').str.starts_with('202306')\n",
    "    )\n",
    "    .group_by('day').agg(pl.col('vessels_avg').sum())\n",
    "    .sort('day'),\n",
    "    y='vessels_avg', x='day',\n",
    "    title='Vessels per day',\n",
    "    labels={'vessels_avg':'Vessel Count'}\n",
    ")\n",
    "\n",
    "#### checking raw AIS data\n",
    "#init list of lazyframes\n",
    "lfs = []\n",
    "#process each parquet file individually into lazyframes\n",
    "for file in glob.glob('ais data/data/ais_clean/*.parquet'):\n",
    "    try:\n",
    "        #check file integrity \n",
    "        pl.scan_parquet(file).collect_schema()\n",
    "        #read file\n",
    "        lf = (\n",
    "            pl.scan_parquet(file)\n",
    "            #keep only observations near 2023 03\n",
    "            .filter(pl.col('time').is_between(pl.datetime(2023,1,1), \n",
    "                                              pl.datetime(2023,12,31))\n",
    "            )\n",
    "            #drop smaller vessels\n",
    "            .filter(pl.col('length')>100)\n",
    "        )\n",
    "        #append to list of lazyframes\n",
    "        lfs.append(lf)\n",
    "    except:\n",
    "        print(f'{file} failed')\n",
    "\n",
    "#collect all lazyframes\n",
    "dfs = pl.collect_all(lfs)\n",
    "\n",
    "#create single dataframe\n",
    "ais_df = (\n",
    "    #concat dfs\n",
    "    pl.concat(dfs, how='diagonal_relaxed')\n",
    "    .sort(['mmsi', 'time'])\n",
    "    #add time since last ping\n",
    "    .with_columns(\n",
    "        time_since_last = (pl.col('time')-pl.col('time').shift()).over('mmsi')\n",
    "        .dt.total_seconds()\n",
    "    )\n",
    ")\n",
    "\n",
    "px.histogram(ais_df\n",
    "             #limit to keep plotly from losing its mind\n",
    "             .limit(10000000)\n",
    "             #filter to pings less than 2hr apart\n",
    "             .filter((pl.col('time_since_last')/60<=120) & (pl.col('status')==5)), \n",
    "             x='time_since_last', nbins=100)\n",
    "\n",
    "px.bar(\n",
    "    ais_df\n",
    "    .filter(pl.col('time_since_last')<45)\n",
    "    .with_columns(\n",
    "        #split by coast\n",
    "        coast = (\n",
    "            pl.when(pl.col('lon')>103)\n",
    "            .then(pl.lit('west'))\n",
    "            .otherwise(pl.lit('east'))\n",
    "        ),\n",
    "        #cast day to dt\n",
    "        day = pl.col('time').dt.strftime('%Y%m%d')\n",
    "    )\n",
    "    .group_by(['coast','day']).agg(\n",
    "        messages = pl.col('mmsi').count(),\n",
    "        avg_time_since_last = pl.col('time_since_last').mean()\n",
    "    )\n",
    "    .with_columns(pl.col('day').str.to_date('%Y%m%d'))\n",
    "    .sort('day'), \n",
    "    y='messages', x='day',\n",
    "    #color='coast'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('AIS_2023_05_25.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = pl.read_csv('AIS_2023_05_09.csv', truncate_ragged_lines=True, infer_schema_length=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polars_lf = (\n",
    "    #load from parquet file - NOTE scan_ tells polars to be in lazy mode\n",
    "    pl.scan_parquet('ais data/data/ais_clean/ais_2023_12.parquet')\n",
    "    #sort by vessel and time\n",
    "    .sort(['mmsi', 'time'])\n",
    "    #drop smaller vessels\n",
    "    .filter(pl.col('length')>100)\n",
    "    #keep necessary columns\n",
    "    .select(['mmsi','time', 'status'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polars_lf.show_graph(optimized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/adamwilson/Library/CloudStorage/OneDrive-WashingtonStateUniversity(email.wsu.edu)/Port Performance/data/AIS/')\n",
    "\n",
    "pl.read_parquet('2015_3.parquet').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit polars_lf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_parquet('ais data/data/ais_clean/ais_2023_12.parquet').to_pandas().to_parquet('ais data/data/file.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "#read parquet\n",
    "df = pd.read_parquet('ais data/data/file.parquet', engine='pyarroe')\n",
    "#sort by mmsi and time\n",
    "df = df.sort_values(by=['mmsi', 'time'])\n",
    "#filter by vessel length\n",
    "df = df[df.length>100]\n",
    "#drop unused columns\n",
    "df = df[['mmsi', 'time', 'status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "#read parquet\n",
    "df = pd.read_parquet('ais data/data/file.parquet')[['mmsi','time', 'status', 'length']]\n",
    "#sort by mmsi and time\n",
    "df = df.sort_values(by=['mmsi', 'time'])\n",
    "#drop unused column\n",
    "df.drop('length', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polars_lf.collect().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame()\n",
    "for file in [f for f in os.listdir('ais data/data/ais_clean/') if not f.startswith('.')]:\n",
    "    file_df = pl.read_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    #read into lazyframe\n",
    "    pl.scan_parquet('ais data/data/ais_clean/*.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load dock data\n",
    "docks_gdf = (\n",
    "    #read in shape file downloaded from USACE\n",
    "    gpd.read_file('port data/Dock/Dock.shp')\n",
    "    #drop unneeded columns\n",
    "    .drop([\n",
    "        'FID', #randomly assigned table id\n",
    "        'LONGITUDE', 'LATITUDE', #already coded in 'geometry' \n",
    "        'LOCATION_D', #text description of dock location\n",
    "        'STREET_ADD','ZIPCODE', #street address details\n",
    "        'PSA_NAME', #statistical area name, rarely used\n",
    "        'COUNTY_NAM', 'COUNTY_FIP', 'CONGRESS', 'CONGRESS_F', #county and congress info\n",
    "        'MILE', 'BANK', 'LATITUDE1', 'LONGITUDE1', #redundant locaation data\n",
    "        'OPERATORS', 'OWNERS', #owner info\n",
    "        'PURPOSE', #long-form text description of dock uses\n",
    "        'DOCK', #unknown number (not unique to each row/dock)\n",
    "        'HIGHWAY_NO', 'RAILWAY_NO', 'LOCATION', #redundant location info\n",
    "        'COMMODITIE', 'CONSTRUCTI','MECHANICAL', 'REMARKS', 'VERTICAL_D', \n",
    "        'DEPTH_MIN', 'DEPTH_MAX','BERTHING_L', 'BERTHING_T', 'DECK_HEIGH', \n",
    "        'DECK_HEI_1', #these are rarely used stats on construction\n",
    "        'SERVICE_IN','SERVICE_TE', #rarely used indicators of data entry date \n",
    "    ], axis=1)\n",
    "    #set coordinate reference system to WGS84 lat/long\n",
    "    .to_crs('EPSG:4326')\n",
    "    #rename cols for clarity\n",
    "    .rename(columns={\n",
    "        'NAV_UNIT_I':'nav_unit_id',\n",
    "        'NAV_UNIT_N':'nav_unit_name',\n",
    "        'FACILITY_T':'facility_type',\n",
    "        'CITY_OR_TO':'city',\n",
    "        'STATE_POST':'state'\n",
    "    })\n",
    ")\n",
    "#set col names to pythonic lowercase\n",
    "docks_gdf.columns = docks_gdf.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.geometry.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree\n",
    "np.random.seed(0)\n",
    "X = np.random.random((5, 2))  # 5 points in 2 dimensions\n",
    "tree = KDTree(X)\n",
    "nearest_dist, nearest_ind = tree.query(X, k=2)  # k=2 nearest neighbors where k1 = identity\n",
    "print(X)\n",
    "print(nearest_dist[:, 1])    # drop id; assumes sorted -> see args!\n",
    "print(nearest_ind[:, 1])     # drop id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "\n",
    "# Set column 'B' to None where column 'A' is equal to 2\n",
    "df.loc[df['A'] == 2, 'B'] = None\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
