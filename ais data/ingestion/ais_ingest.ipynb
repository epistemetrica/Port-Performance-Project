{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIS Data Ingestion \n",
    "\n",
    "Vessel locations data is ingested from the Automatic Identification System (AIS) data available from the federal [Marine Cadastre website](https://hub.marinecadastre.gov/pages/vesseltraffic), and is processed in the following steps:\n",
    "- read data from the csv urls corresponding to each calendar day\n",
    "- drop unnessary columns\n",
    "- filter to only include cargo vessels\n",
    "- cast datatypes appropriately\n",
    "- save daily file to parquet\n",
    "\n",
    "Notes:\n",
    "- failed downloads are noted in print outputs\n",
    "- rows that would cause parsing errors (e.g., if there is an extra comma on one row) are skipped and are thus missing from the saved outputs. A warning is printed in the cell output.\n",
    "\n",
    "Descriptions of each column of the raw data are available at the [AIS Data Dictionary](https://coast.noaa.gov/data/marinecadastre/ais/data-dictionary.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preliminaries\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "#enable string cache for polars categoricals\n",
    "pl.enable_string_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set variables\n",
    " \n",
    "#start and end dates (format yyyy_mm_dd)\n",
    "start_date = '2018_01_01'\n",
    "end_date = '2024_03_31'\n",
    "\n",
    "#vessel types - includes cargo and tanker types\n",
    "cargo_types = pl.arange(70,90,eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init days as series \n",
    "days = pl.date_range(datetime.strptime(start_date, '%Y_%m_%d'),\n",
    "                  datetime.strptime(end_date, '%Y_%m_%d'), eager=True)\n",
    "\n",
    "#define processing function\n",
    "def process(day, replace=False):\n",
    "    #get year\n",
    "    year = day.year\n",
    "    #convert day to string\n",
    "    day = day.strftime('%Y_%m_%d')\n",
    "    #load from url to pandas df\n",
    "    try:\n",
    "        day_raw = (\n",
    "            pd.read_csv(\n",
    "                f'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/{year}/AIS_{day}.zip',\n",
    "                low_memory=False, \n",
    "                #print warning and skip row when parsing error is encountered\n",
    "                on_bad_lines='warn'\n",
    "            )\n",
    "        )\n",
    "        print(f'Download complete for {day}.')\n",
    "        try:\n",
    "            #convert to polars\n",
    "            day_df = pl.DataFrame(day_raw,infer_schema_length=0)\n",
    "            #process data\n",
    "            day_df = (\n",
    "                day_df\n",
    "                #keep only cargo vessels\n",
    "                .filter(pl.col('VesselType').is_in(cargo_types))\n",
    "                #keep cols of interest\n",
    "                .select('MMSI', 'BaseDateTime','LAT', 'LON', 'SOG', 'COG', \n",
    "                        'Heading', 'Status', 'VesselName', 'VesselType', 'IMO',\n",
    "                        'Length', 'Width', 'Draft','Cargo')\n",
    "                #give pythonic names\n",
    "                .rename({\n",
    "                    'MMSI':'mmsi',\n",
    "                    'BaseDateTime':'time',\n",
    "                    'LAT':'lat',\n",
    "                    'LON':'lon',\n",
    "                    'SOG':'speed',\n",
    "                    'COG':'course',\n",
    "                    'Heading':'heading',\n",
    "                    'Status':'status',\n",
    "                    'VesselName':'vessel_name',\n",
    "                    'VesselType':'vessel_type',\n",
    "                    'IMO':'imo',\n",
    "                    'Length':'length',\n",
    "                    'Width':'width',\n",
    "                    'Draft':'draft',\n",
    "                    'Cargo':'cargo'\n",
    "                })\n",
    "                #clean cols\n",
    "                .with_columns(\n",
    "                    #strip IMO prefix and cast to int\n",
    "                    imo = pl.col('imo').str.strip_prefix('IMO').cast(pl.Int64),\n",
    "                    #clean course and heading \n",
    "                    course = pl.col('course').replace(360.0,None),\n",
    "                    heading = pl.col('heading').replace(511.0,None)\n",
    "                )\n",
    "                #cast\n",
    "                .cast({\n",
    "                    'time':pl.Datetime,\n",
    "                    'vessel_name':pl.Categorical\n",
    "                })\n",
    "                #deduplicate\n",
    "                .unique()\n",
    "            )\n",
    "            #write processed file to parquet\n",
    "            day_df.write_parquet(f'../data/ais_clean/ais_{day}.parquet')\n",
    "            print(f'{day} successfully processed and saved to parquet.')\n",
    "        except:\n",
    "            try:\n",
    "                #write raw file to csv \n",
    "                day_raw.to_csv(f'../data/ais_processing_errors/ais_{day}.csv')\n",
    "                print(f'WARNING: Error in processing {day} data. Raw file saved to CSV instead.' )\n",
    "            except:\n",
    "                print(f'WARNING: Error in processing {day} data. FILE LOST.')\n",
    "    except:\n",
    "        print(f'Error importing {day} to dataframe - Url may be invalid or download may have failed.')\n",
    "    \n",
    "#define main function\n",
    "def ais_ingest(days=days, replace=False):\n",
    "    if replace:\n",
    "        for day in days:\n",
    "            process(day)\n",
    "    else:\n",
    "        for day in days:\n",
    "            if not os.path.exists(f'../data/ais_clean/ais_{day.strftime('%Y_%m_%d')}.parquet'):\n",
    "                process(day)\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/sgfd2dls28n4y4v4_jw03ly40000gp/T/ipykernel_77094/2270074044.py:14: ParserWarning: Skipping line 8611748: expected 17 fields, saw 18\n",
      "\n",
      "  pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete for 2020_07_11.\n",
      "2020_07_11 successfully processed and saved to parquet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/sgfd2dls28n4y4v4_jw03ly40000gp/T/ipykernel_77094/2270074044.py:14: ParserWarning: Skipping line 8728984: expected 17 fields, saw 18\n",
      "\n",
      "  pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete for 2020_07_16.\n",
      "2020_07_16 successfully processed and saved to parquet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/sgfd2dls28n4y4v4_jw03ly40000gp/T/ipykernel_77094/2270074044.py:14: ParserWarning: Skipping line 8306302: expected 17 fields, saw 18\n",
      "\n",
      "  pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete for 2020_07_30.\n",
      "2020_07_30 successfully processed and saved to parquet.\n",
      "Download complete for 2020_08_04.\n",
      "2020_08_04 successfully processed and saved to parquet.\n",
      "Download complete for 2020_08_05.\n",
      "2020_08_05 successfully processed and saved to parquet.\n",
      "Download complete for 2020_08_12.\n",
      "2020_08_12 successfully processed and saved to parquet.\n",
      "Download complete for 2020_08_17.\n",
      "2020_08_17 successfully processed and saved to parquet.\n"
     ]
    }
   ],
   "source": [
    "#run main function\n",
    "ais_ingest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
